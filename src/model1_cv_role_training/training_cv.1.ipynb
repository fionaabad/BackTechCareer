{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e91ac02d",
   "metadata": {},
   "source": [
    "# Training de Modelo 1.1. – Clasificador de CV con BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6b829fb",
   "metadata": {},
   "source": [
    "Qué es BERT?\n",
    "\n",
    "BERT (Bidirectional Encoder Representations from Transformers) es un modelo **transformer preentrenado** en grandes cantidades de texto.\n",
    "“Bidirectional” significa que lee el texto **tanto hacia adelante como hacia atrás**, captando mejor el contexto.\n",
    "Se puede **fine-tunear** para tareas específicas, como clasificación de textos, sin necesidad de entrenarlo desde cero.\n",
    "\n",
    "**Ventajas para CVs:**\n",
    "\n",
    "* Entiende mejor frases y contexto que TF-IDF.\n",
    "* Puede captar relaciones entre frases y palabras clave (por ejemplo, “Python” + “Data Science”).\n",
    "* Mejora la generalización y evita tanto el overfitting como la pérdida de semántica.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c92e47a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "print(requests.get(\"https://huggingface.co\").status_code)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d6b2a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "from transformers import (\n",
    "    TFAutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d37ea8",
   "metadata": {},
   "source": [
    "Importación del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dacb1f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('/Users/alexmiliandieguez/code/TechCareer/data/model1_cv_role/3.processed/v1_english/cv_labeled_final.csv')\n",
    "df = df.loc[:, ['cv_text', 'role_label_final']]\n",
    "df.rename(columns={'cv_text': 'cv', 'role_label_final': 'role'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1c0bf4",
   "metadata": {},
   "source": [
    "Codificar etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e445c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "df['role_encoded'] = le.fit_transform(df['role'])\n",
    "num_classes = len(le.classes_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dd9491",
   "metadata": {},
   "source": [
    "Train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "89245641",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['cv'].values,\n",
    "    df['role_encoded'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['role_encoded']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bca0ab",
   "metadata": {},
   "source": [
    "Modelo y tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "65b5e408",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/bert_uncased_L-6_H-512_A-8\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e1b18f",
   "metadata": {},
   "source": [
    "Modelo BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5ecf5f28",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m num_classes = \u001b[38;5;28mlen\u001b[39m(le.classes_)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m model = \u001b[43mTFAutoModelForSequenceClassification\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfrom_pt\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# <-- aquí indiquem que carregui pesos PyTorch\u001b[39;49;00m\n\u001b[32m      7\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/TechCareer/.venv/lib/python3.11/site-packages/transformers/models/auto/auto_factory.py:604\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    602\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m model_class.config_class == config.sub_configs.get(\u001b[33m\"\u001b[39m\u001b[33mtext_config\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m--> \u001b[39m\u001b[32m604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel_class\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/TechCareer/.venv/lib/python3.11/site-packages/transformers/modeling_tf_utils.py:2941\u001b[39m, in \u001b[36mTFPreTrainedModel.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001b[39m\n\u001b[32m   2938\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodeling_tf_pytorch_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_pytorch_checkpoint_in_tf2_model\n\u001b[32m   2940\u001b[39m     \u001b[38;5;66;03m# Load from a PyTorch checkpoint\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2941\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_pytorch_checkpoint_in_tf2_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2942\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresolved_archive_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_missing_keys\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   2945\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_loading_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2946\u001b[39m \u001b[43m        \u001b[49m\u001b[43m_prefix\u001b[49m\u001b[43m=\u001b[49m\u001b[43mload_weight_prefix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2947\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtf_to_pt_weight_rename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2948\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2950\u001b[39m \u001b[38;5;66;03m# we might need to extend the variable scope for composite models\u001b[39;00m\n\u001b[32m   2951\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m load_weight_prefix \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/TechCareer/.venv/lib/python3.11/site-packages/transformers/modeling_tf_pytorch_utils.py:205\u001b[39m, in \u001b[36mload_pytorch_checkpoint_in_tf2_model\u001b[39m\u001b[34m(tf_model, pytorch_checkpoint_path, tf_inputs, allow_missing_keys, output_loading_info, _prefix, tf_to_pt_weight_rename)\u001b[39m\n\u001b[32m    203\u001b[39m     state_dict = safe_load_file(pt_path)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m205\u001b[39m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    206\u001b[39m     state_dict = torch.load(pt_path, map_location=\u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m, weights_only=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    208\u001b[39m pt_state_dict.update(state_dict)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/code/TechCareer/.venv/lib/python3.11/site-packages/transformers/utils/import_utils.py:1647\u001b[39m, in \u001b[36mcheck_torch_load_is_safe\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1645\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcheck_torch_load_is_safe\u001b[39m() -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1646\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[33m\"\u001b[39m\u001b[33m2.6\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1647\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1648\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1649\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1650\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwhen loading files with safetensors.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1651\u001b[39m             \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1652\u001b[39m         )\n",
      "\u001b[31mValueError\u001b[39m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
     ]
    }
   ],
   "source": [
    "num_classes = len(le.classes_)\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_classes,\n",
    "    from_pt=True  # <-- aquí indiquem que carregui pesos PyTorch\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2a7d5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TechCareer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
